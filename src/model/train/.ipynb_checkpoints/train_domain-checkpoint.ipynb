{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on images\n",
    "\n",
    "Domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom scripts\n",
    "from dataset import *\n",
    "from utils import *\n",
    "import config \n",
    "\n",
    "# import the necessary packages\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from imutils import paths\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss \n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from utils import EarlyStopping\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "import time\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling sources of randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "SEED = 42\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "[INFO] CUDA version: 11.1\n",
      "[INFO] ID of current CUDA device:0\n",
      "[INFO] Name of current CUDA device:NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"[INFO] CUDA version: {torch.version.cuda}\")\n",
    "  \n",
    "# storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"[INFO] ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "        \n",
    "print(f\"[INFO] Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving testing image paths...\n"
     ]
    }
   ],
   "source": [
    "# load the image and label filepaths in a sorted manner\n",
    "imagePaths = sorted(list(paths.list_images(config.IMAGE_DATASET_PATH)))\n",
    "labelPaths = sorted(list(paths.list_images(config.LABEL_DATASET_PATH)))\n",
    "\n",
    "trainImages_source, testImages, trainLabels_source, testLabels = train_test_split(imagePaths,\n",
    "                                                                    labelPaths,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    train_size=0.8,\n",
    "                                                                    random_state=SEED)\n",
    "testImages, valImages, testLabels, valLabels = train_test_split(testImages, \n",
    "                                                                  testLabels,\n",
    "                                                                  test_size=0.5,\n",
    "                                                                  train_size=0.5,\n",
    "                                                                  random_state=SEED)\n",
    "\n",
    "# get the masks corresponding to the labels\n",
    "trainMasks = [s.replace('labels', 'masks_9x9') for s in trainLabels_source]\n",
    "valMasks = [s.replace('labels', 'masks_9x9') for s in valLabels]\n",
    "\n",
    "# save testing images to disk\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "f = open(config.TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(testImages))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = sorted(list(paths.list_images(\"/data/jantina/CoralNet/inference/images/train/\")))\n",
    "labelPaths = sorted(list(paths.list_images(\"/data/jantina/CoralNet/inference/labels/train/\")))\n",
    "\n",
    "trainImages_target, valImages, trainLabels_target, valLabels = train_test_split(imagePaths,\n",
    "                                                                  labelPaths,\n",
    "                                                                  test_size=0.2,\n",
    "                                                                  train_size=0.8,\n",
    "                                                                  random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_source = A.Compose([\n",
    "    A.RandomResizedCrop(width=128, height=128, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=cv2.INTER_NEAREST),    \n",
    "    A.HorizontalFlip(p=0.5),              \n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5),\n",
    "    A.CLAHE(p=0.8),\n",
    "    A.RandomBrightnessContrast(p=0.8),    \n",
    "    A.RandomGamma(p=0.8),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_target = A.Compose([\n",
    "    A.RandomResizedCrop(width=128, height=128, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=cv2.INTER_NEAREST),    \n",
    "    A.HorizontalFlip(p=0.5),              \n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5),\n",
    "    A.CLAHE(p=0.8),\n",
    "    A.RandomBrightnessContrast(p=0.8),    \n",
    "    A.RandomGamma(p=0.8),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 6829 examples in the source set\n",
      "[INFO] found 46 examples in the target set\n",
      "[INFO] total time taken to load the data: 0.00s\n"
     ]
    }
   ],
   "source": [
    "# create the train and validation datasets\n",
    "startTime = time.time()\n",
    "train_source = Dataset2(imagePaths=trainImages_source, labelPaths = trainLabels_source, transform=transform_source)\n",
    "train_target = Dataset2(imagePaths=trainImages_target, labelPaths = trainLabels_target, transform=transform_target)\n",
    "\n",
    "\n",
    "print(f\"[INFO] found {len(train_source)} examples in the source set\")\n",
    "print(f\"[INFO] found {len(train_target)} examples in the target set\")\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to load the data: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_batch = config.BATCH_SIZE // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training and validation data loaders\n",
    "source_loader = DataLoader(train_source, shuffle=True, \n",
    "                         batch_size=config.BATCH_SIZE, \n",
    "                         pin_memory=config.PIN_MEMORY, \n",
    "                         num_workers=os.cpu_count(),\n",
    "                         persistent_workers=True,\n",
    "                         worker_init_fn=seed_worker)\n",
    "\n",
    "target_loader = DataLoader(train_target, shuffle=False, \n",
    "                       batch_size=config.BATCH_SIZE, \n",
    "                       pin_memory=config.PIN_MEMORY, \n",
    "                       num_workers=os.cpu_count(),\n",
    "                       persistent_workers=True,\n",
    "                       worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pretrained model\n",
    "unet = torch.load(\"/data/jantina/CoralNet/dataset/output/weighted.pth\").to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = unet.encoder\n",
    "clf = unet.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    GradientReversal(),\n",
    "    nn.Linear(320, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss function\n",
    "lossFunc = CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# initialize optimizer\n",
    "opt = Adam(list(discriminator.parameters()) + list(unet.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet()\n",
    "Discriminator = MLP(512, 256, n_domain_classes)\n",
    "\n",
    "for (Img, label, domain_label) in Dataloader:\n",
    "\n",
    "    prediction = model(Img)\n",
    "\n",
    "\n",
    "    # img of shape: Bx3x128x128\n",
    "    encoding = model.encoder(Img)\n",
    "    # encoding of shape: Bx512x15x15\n",
    "\n",
    "    encoding_global = encoding.mean(dim=2).mean(dim=3)\n",
    "    #encoding_global of shape: Bx512\n",
    "\n",
    "    domain_prediction = Discriminator(encoding_global)\n",
    "\n",
    "    segmentation_loss = crossentropy(predictiomn, label)\n",
    "    domain_loss = crossentropy(domain_prediction, domain_label)\n",
    "\n",
    "    loss = added loss with flipped gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m domain_y \u001b[38;5;241m=\u001b[39m domain_y\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m     16\u001b[0m label_y \u001b[38;5;241m=\u001b[39m source_labels\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[0;32m---> 18\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m domain_preds \u001b[38;5;241m=\u001b[39m discriminator(features)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     20\u001b[0m label_preds \u001b[38;5;241m=\u001b[39m clf(features[:source_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5+1):\n",
    "    batches = zip(source_loader, target_loader)\n",
    "    n_batches = min(len(source_loader), len(target_loader))\n",
    "\n",
    "    total_domain_loss = total_label_accuracy = 0\n",
    "    \n",
    "    for (source_x, source_labels), (target_x, target_labels) in tqdm(batches, leave=False, total=n_batches):\n",
    "            x = torch.cat([source_x, target_x])\n",
    "            x = x.to(config.DEVICE)\n",
    "            \n",
    "            domain_y = torch.cat([torch.ones(source_x.shape[0]),\n",
    "                                  torch.zeros(target_x.shape[0])])\n",
    "            domain_y = domain_y.to(config.DEVICE)\n",
    "            label_y = source_labels.to(config.DEVICE)\n",
    "\n",
    "            features = feature_extractor(x).view(x.shape[0], -1)\n",
    "            domain_preds = discriminator(features).squeeze()\n",
    "            label_preds = clf(features[:source_x.shape[0]])\n",
    "\n",
    "            domain_loss = lossFunc(domain_preds, domain_y)\n",
    "            label_loss = lossFunc(label_preds, label_y)\n",
    "            loss = domain_loss + label_loss\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_domain_loss += domain_loss.item()\n",
    "            total_label_accuracy += (label_preds.max(1)[1] == label_y).float().mean().item()\n",
    "\n",
    "    mean_loss = total_domain_loss / n_batches\n",
    "    mean_accuracy = total_label_accuracy / n_batches\n",
    "    tqdm.write(f'EPOCH {epoch:03d}: domain_loss={mean_loss:.4f}, '\n",
    "               f'source_accuracy={mean_accuracy:.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'trained_models/revgrad.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
