{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom scripts\n",
    "from dataset_tensors import Dataset\n",
    "import config\n",
    "\n",
    "# import the necessary packages\n",
    "from torch.nn import CrossEntropyLoss \n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "[INFO] CUDA version: 11.1\n",
      "[INFO] ID of current CUDA device:0\n",
      "[INFO] Name of current CUDA device:NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"[INFO] CUDA version: {torch.version.cuda}\")\n",
    "  \n",
    "# storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"[INFO] ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "        \n",
    "print(f\"[INFO] Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tensor paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images train\n",
    "#directory = '/data/jantina/CoralNet/WAPA_RFM/small_dataset/images_tensor_small/train/'\n",
    "directory = '/data/jantina/CoralNet/WAPA_RFM/images_tensor/train/'\n",
    "trainImages = []\n",
    "for filename in os.listdir(directory):\n",
    "    trainImages.append((directory+filename))\n",
    "    \n",
    "# Images test\n",
    "#directory = '/data/jantina/CoralNet/WAPA_RFM/small_dataset/images_tensor_small/test/'\n",
    "directory = '/data/jantina/CoralNet/WAPA_RFM/images_tensor/test/'\n",
    "testImages = []\n",
    "for filename in os.listdir(directory):\n",
    "    testImages.append((directory+filename))\n",
    "    \n",
    "# Images train\n",
    "#directory = '/data/jantina/CoralNet/WAPA_RFM/small_dataset/labels_tensor_small/train/'\n",
    "directory = '/data/jantina/CoralNet/WAPA_RFM/labels_tensor/train/'\n",
    "trainMasks = []\n",
    "for filename in os.listdir(directory):\n",
    "    trainMasks.append((directory+filename))\n",
    "    \n",
    "# Images test\n",
    "#directory = '/data/jantina/CoralNet/WAPA_RFM/small_dataset/labels_tensor_small/test/'\n",
    "directory = '/data/jantina/CoralNet/WAPA_RFM/labels_tensor/test/'\n",
    "testMasks = []\n",
    "for filename in os.listdir(directory):\n",
    "    testMasks.append((directory+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 2160 examples in the training set\n",
      "[INFO] found 382 examples in the test set\n",
      "[INFO] total time taken to load the data: 0.00s\n"
     ]
    }
   ],
   "source": [
    "# create the train and test datasets\n",
    "startTime = time.time()\n",
    "trainDS = Dataset(imagePaths=trainImages, maskPaths=trainMasks)\n",
    "testDS = Dataset(imagePaths=testImages, maskPaths=testMasks)\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the training set\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the test set\")\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to load the data: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jantina/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# create the training and test data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True, \n",
    "                         batch_size=config.BATCH_SIZE, \n",
    "                         pin_memory=config.PIN_MEMORY, \n",
    "                         num_workers=64,\n",
    "                         persistent_workers=True\n",
    "                        )\n",
    "\n",
    "testLoader = DataLoader(testDS, shuffle=False, \n",
    "                        batch_size=config.BATCH_SIZE, \n",
    "                        pin_memory=config.PIN_MEMORY, \n",
    "                        num_workers=64,\n",
    "                        persistent_workers=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = trainDS[0]\n",
    "print(f\"Feature shape: {sample[0].shape}\")\n",
    "print(f\"Labels shape: {sample[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                  transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ])])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "image = sample[0]\n",
    "image = torch.as_tensor(image, dtype=torch.float32)\n",
    "ax[0].imshow(unnormalize(image).permute(1, 2, 0))\n",
    "ax[1].imshow(sample[1], cmap = \"tab20\", vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a pretrained model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,      \n",
    "    classes=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = model.to(config.DEVICE)\n",
    "\n",
    "# initialize loss function\n",
    "lossFunc = CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "# initialize optimizer\n",
    "opt = Adam(unet.parameters(), lr=config.INIT_LR)\n",
    "\n",
    "# calculate steps per epoch for training and test set\n",
    "trainSteps = len(trainDS) // config.BATCH_SIZE\n",
    "testSteps = len(testDS) // config.BATCH_SIZE\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "\n",
    "unnormalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                  transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ])])\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(config.NUM_EPOCHS)):\n",
    "    # set the model in training mode\n",
    "    unet.train()\n",
    "\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalTestLoss = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    for (i, (x, y)) in enumerate(trainLoader):\n",
    "        # send the input to the device\n",
    "        (x, y) = (torch.as_tensor(x, dtype=torch.float).to(config.DEVICE),\n",
    "                  torch.as_tensor(y, dtype=torch.long).to(config.DEVICE))\n",
    "        \n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = unet(x)\n",
    "        loss = lossFunc(pred, y)\n",
    "        \n",
    "        # first, zero out any previously accumulated gradients, then\n",
    "        # perform backpropagation, and then update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # add the loss to the total training loss so far\n",
    "        totalTrainLoss += loss.item()\n",
    "        \n",
    "    \n",
    "    # switch off autograd\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        unet.eval()\n",
    "        \n",
    "        # loop over the validation set\n",
    "        for (x, y) in testLoader:\n",
    "            # send the input to the device\n",
    "            (x, y) = (torch.as_tensor(x, dtype=torch.float).to(config.DEVICE),\n",
    "                      torch.as_tensor(y, dtype=torch.long).to(config.DEVICE))\n",
    "            \n",
    "            \n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = unet(x)\n",
    "            totalTestLoss += lossFunc(pred, y).item()\n",
    "            \n",
    "            fig, ax = plt.subplots(ncols=3)\n",
    "            ax[0].imshow(unnormalize(x[0]).cpu().detach().permute(1, 2, 0))\n",
    "            ax[1].imshow(y[0].cpu(), cmap = \"tab20\", vmin=0, vmax=255)\n",
    "            ax[2].imshow(torch.argmax(pred[0],dim=0).cpu(), cmap = \"tab20\", vmin=0, vmax=255)\n",
    "            plt.show()\n",
    "            \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgTestLoss = totalTestLoss / testSteps\n",
    "    \n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss)\n",
    "    H[\"test_loss\"].append(avgTestLoss)\n",
    "    \n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, config.NUM_EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(avgTrainLoss, avgTestLoss))\n",
    "    \n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim((0,20))\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(config.PLOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "torch.save(unet, config.MODEL_PATH)\n",
    "print(\"[INFO] model saved !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
